---
title: "3-3 최적화 알고리즘"
output: html_notebook
---

## 0. 데이터셋 소개

### Wild blueberry

[Wild blueberry](https://data.mendeley.com/datasets/p5hvjzsvn8/1) 야생 블루베리의 수확량을 시뮬레이션한 데이터셋입니다.


|Features|Unit|Description|
|:--------|:----|:-----------|
|Row#||Index|
|clonesize|m2|The average blueberry clone size in the field|
|honeybee|bees/m2/min|Honeybee density in the field|
|bumbles|bees/m2/min|Bumblebee density in the field|
|andrena|bees/m2/min|Andrena bee density in the field|
|osmia|bees/m2/min|Osmia bee density in the field|
|MaxOfUpperTRange|℃|The highest record of the upper band daily air temperature during the bloom season|
|MinOfUpperTRange|℃|The lowest record of the upper band daily air temperature|
|AverageOfUpperTRange|℃|The average of the upper band daily air temperature|
|MaxOfLowerTRange|℃|The highest record of the lower band daily air temperature|
|MinOfLowerTRange|℃|The lowest record of the lower band daily air temperature|
|AverageOfLowerTRange|℃|The average of the lower band daily air temperature|
|RainingDays|Day|The total number of days during the bloom season, each of which has precipitation larger than zero|
|AverageRainingDays|Day|The average of raining days of the entire bloom season|
|fruitset||The amount of fruit per tree|
|fruitmass||The mass of a fruit|
|seeds||The mass of a seed|
|yield||The amount of yield|
 
```{r}
library(readr)
# CSV 파일 읽기
df_berry <- read_csv('../data/wild_blueberry.csv')

# 데이터 출력
print(head(df_berry))
```

## 1. 경사 하강법(Gradient Descent)

**아이디어**

> 목적 함수의 최적값을 찾기 위해 목적 함수에 대한 파라메터(Parameter)의 경사(Gradient)을 이용합니다.
>
> 목적 함수의 최적값의 방향으로 파라메터(Parameter)를 경사도에 따라 이동시킵니다.
>
> 경사도를 구하고 이동을 반복하여 파라메터의 최적점을 도출합니다.

### 알고리즘

1. 파라메터($\hat{\beta}$) 초기화


2. 파라메터 경사도($\frac{\partial{L}}{\partial{\hat{\beta}}}$) 계산, L: 손실, 목적함수


3. 파라메터 업데이트

> $\hat{\beta} = \hat{\beta} - \text{Learning rating} × \frac{\partial{L}}{\partial{\hat{\beta}}}$, 

> Learning rating : 경사도 반영율

4. 종료 조건 미충족 시 2번 부터 반복

> 종료 조건: 업데이트 반복횟수(epochs), 성능 개선 정도, ...

**[Ex.1]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px;">
  
$y = X + 5 + e, e \sim N(0, 1)$
    
관계를 지닌 데이터 포인트 100개를 만듭니다.
    
경사하강법의 동작 원리를 파악하기 위해, 기울기와 절편을 경사 하강법을 이용한 선형 회귀 모델 학습합니다.

여기서, 회귀 모델의 손실 함수(Loss Function)를 평균제곱오차(Mean Squared Error)를 이용하여

회귀 계수와 손실 함수 경사도를 구합니다.

$L = \frac{1}{n}\sum_{i=1}^n\left(y_i - \hat{y_i}\right)^2 $

> $\hat{y} = X\hat{\beta},  r = y - \hat{y}$

$\frac{dL}{d\hat{\beta}} =  -\frac{2}{n}X^Tr$
    
</div>

```{r}
# 단순 선형 회귀 데이터셋을 만듭니다.

beta_1 <- 1
beta_0 <- 5  # 종속변수 (y) 독립변수(X) 간의 기울기와 절편입니다.

# 동일한 실행 결과를 얻게하기 위해 랜덤 시드값을 고정시킵니다.
set.seed(123)

# 기울기는 beta_1, 절편은 beta_0이고 오차가 표준정규분포인 임의의 100개의 데이터 포인트를 생성합니다.
X <- runif(100, min = 0, max = 10)
y <- X * beta_1 + beta_0 + rnorm(100)

# 플롯을 생성합니다.
library(ggplot2)

ggplot(data.frame(X, y), aes(x = X, y = y)) +
  geom_point(size = 2) +
  labs(x = "X", y = "y") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
# 상수항을 추가합니다.
X_add_const <- cbind(1, X)

# 앞의 5개 행을 출력합니다.
head(X_add_const, 5)
```
```{r}
# 파라메터를 초기화 합니다.
set.seed(123)  # 동일한 실행 결과를 위해 랜덤 시드값을 고정합니다.
beta_hat <- runif(2, min = -1, max = 1)
beta_hat
```
경사도를 구하는 부분만 가져옵니다.

$L = \frac{1}{n}\sum_{i=1}^n\left(y_i - \hat{y_i}\right)^2 $

> $\hat{y} = X\hat{\beta},  r = y - \hat{y}$

$\frac{dL}{d\hat{\beta}} =  -\frac{2}{n}X^Tr$


```{r}

epochs <- 500  # 파라미터 업데이트 루틴의 반복 횟수입니다.
learning_rate <- 0.01  # 파라미터 업데이트 비율입니다.
n <- nrow(X_add_const)
losses <- c()
beta_hats <- list()

# 경사 하강법을 사용하여 파라미터 업데이트
for (i in 1:epochs) {
  residuals <- y - as.vector(X_add_const %*% beta_hat)
  losses <- c(losses, mean(residuals^2))
  
  # 100 epoch마다 모수를 기록합니다.
  if (i %% 100 == 0) {
    beta_hats[[length(beta_hats) + 1]] <- beta_hat
  }
  
  beta_hat_grad <- -2 * (t(X_add_const) %*% residuals) / n
  beta_hat <- beta_hat - learning_rate * beta_hat_grad
}

# 최종 모수를 기록합니다.
beta_hats[[length(beta_hats) + 1]] <- beta_hat

# 최종 모수와 손실 확인
beta_hats
losses[length(losses)]
```
```{r}
# 모델 시각화
plot_data <- data.frame(X = X, y = y)

# 초기 모델
initial_model <- data.frame(
  x = c(0, 10),
  y = beta_hats[[1]][1] + beta_hats[[1]][2] * c(0, 10),
  type = "Initial model"
)

# 중간 모델들
middle_models <- do.call(rbind, lapply(beta_hats[2:(length(beta_hats) - 1)], function(beta) {
  data.frame(
    x = c(0, 10),
    y = beta[1] + beta[2] * c(0, 10),
    type = "Middle model"
  )
}))

# 최종 모델
final_model <- data.frame(
  x = c(0, 10),
  y = beta_hats[[length(beta_hats)]][1] + beta_hats[[length(beta_hats)]][2] * c(0, 10),
  type = "Final model"
)

# 모든 모델을 합침
all_models <- rbind(initial_model, middle_models, final_model)

# 시각화
ggplot(plot_data, aes(x = X, y = y)) +
  geom_point(size = 2) +
  geom_line(data = all_models, aes(x = x, y = y, color = type), size = 1) +
  labs(x = "X", y = "y") +
  scale_color_manual(values = c("Initial model" = "blue", "Middle model" = "orange", "Final model" = "red")) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  ggtitle("Progress of Gradient Descent")
```

```{r}
beta_hats[[6]]
```
```{r}
# 손실 데이터 생성
losses_data <- data.frame(epoch = 1:length(losses), loss = losses)

# 손실을 시각화
ggplot(losses_data, aes(x = epoch, y = loss)) +
  geom_line() +
  labs(x = "Epoch", y = "Loss (MSE)") +
  theme_minimal() +
  ggtitle("Loss per Epoch")
```

```{r}
print(beta_hat_grad)
print(beta_hat)
```
### 학습율 (Learning Rate)

경사 하강법의 안정적이고 효율적인 동작을 위해서는 적절한 학습율이 필요합니다.

- 학습율이 너무 크면 모수가 수렴하지 않고 발산합니다.


- 학습율이 너무 작으면 모수가 수렴하기까지 많은 업데이트가 요구됩니다.

**[Ex.2]**

학습율(Learning rate)를 아래와 같이 달리하면서, epoch 에 따라 손실(Loss)의 변화를 살펴 봅니다.

```
    learning_rates = [0.04, 0.01, 0.001]
```

```{r}

library(gridExtra)

# 학습률과 에포크 설정
epochs <- 100
learning_rates <- c(0.04, 0.01, 0.001)
n <- nrow(X_add_const)

# 각 학습률에 따른 손실을 저장할 리스트 초기화
losses_list <- list()

# 파라미터 초기화
set.seed(123)
initial_beta_hat <- runif(2, min = -1, max = 1)

for (learning_rate in learning_rates) {
  losses <- numeric(epochs)
  beta_hat <- initial_beta_hat
  for (i in 1:epochs) {
    residuals <- y - as.vector(X_add_const %*% beta_hat)
    losses[i] <- mean(residuals^2)
    beta_hat_grad <- -2 * (t(X_add_const) %*% residuals) / n
    beta_hat <- beta_hat - learning_rate * beta_hat_grad
  }
  losses_list[[as.character(learning_rate)]] <- data.frame(epoch = 1:epochs, loss = losses, learning_rate = as.factor(learning_rate))
}

# 손실 데이터를 데이터프레임으로 변환
losses_data <- do.call(rbind, losses_list)

# 각 학습률에 따른 손실 변화를 시각화
p1 <- ggplot(subset(losses_data, learning_rate == '0.04'), aes(x = epoch, y = loss)) +
  geom_line() +
  labs(title = "Learning rate: 0.04", x = "epoch", y = "MSE") +
  theme_minimal()

p2 <- ggplot(subset(losses_data, learning_rate == '0.01'), aes(x = epoch, y = loss)) +
  geom_line() +
  labs(title = "Learning rate: 0.01", x = "epoch", y = "MSE") +
  theme_minimal()

p3 <- ggplot(subset(losses_data, learning_rate == '0.001'), aes(x = epoch, y = loss)) +
  geom_line() +
  labs(title = "Learning rate: 0.001", x = "epoch", y = "MSE") +
  theme_minimal()

# 그래프들을 하나의 레이아웃에 배치
grid.arrange(p1, p2, p3, ncol = 3)
```
### 연속형 변수의 스케일 균일화

다변수 모델에서 변수마다 스케일이 제각각이라면 적합한 변수 별로 학습율 또한 편차가 큽니다.

**경사하강법**이 보다 빠르고 안정적으로 결과를 내려면, 입력 변수 간의 스케일이 고르게 되어 있어야 합니다.

- 연속형 입력 변수들을 표준화나 정규화를 통한 균일화 합니다.

**[Ex.3]** 

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px;">

df_berry 에서 대상 변수를 yield, 입력 변수를 seeds, fruitset, fruitmass로 합니다.

입력 변수의 스케일을 확인하고, 표준화 유무에 따른 경사하강법의 진행 양상을 살펴봅니다.
    
</div>

```{r}
# 각 변수에 대한 히스토그램 생성
p1 <- ggplot(df_berry, aes(x = seeds)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black", alpha = 0.7) +
  labs(x = "Seeds", y = "Count") +
  theme_minimal()

p2 <- ggplot(df_berry, aes(x = fruitset)) +
  geom_histogram(fill = "green", color = "black", alpha = 0.7) +
  labs(x = "Fruitset", y = "Count") +
  theme_minimal()

p3 <- ggplot(df_berry, aes(x = fruitmass)) +
  geom_histogram(fill = "red", color = "black", alpha = 0.7) +
  labs(x = "Fruitmass", y = "Count") +
  theme_minimal()

# 그래프들을 하나의 레이아웃에 배치
grid.arrange(p1, p2, p3, ncol = 3)
```
```{r}
library(dplyr)

# 선택한 변수들
X_cols <- c("seeds", "fruitset", "fruitmass")

# 상수항을 포함시켜 절편을 계산합니다.
df_berry_const <- df_berry %>%
  mutate(const = 1) %>%
  select(X_cols, const)

# 앞의 5개 행을 출력합니다.
head(df_berry_const, 5)
```
```{r}
# 학습률과 에포크 설정
epochs <- 50
learning_rates <- c(1e-3, 1e-4, 1e-5)
n <- nrow(df_berry_const)

# 각 학습률에 따른 손실을 저장할 리스트 초기화
losses_list <- list()

for (learning_rate in learning_rates) {
  losses <- numeric(epochs)
  set.seed(123)
  beta_hat <- runif(length(X_cols) + 1, min = -1, max = 1)
  for (i in 1:epochs) {
    residuals <- df_berry$yield - as.vector(as.matrix(df_berry_const) %*% beta_hat)
    losses[i] <- mean(residuals^2)
    beta_hat_grad <- -2 * (t(as.matrix(df_berry_const)) %*% residuals) / n
    beta_hat <- beta_hat - learning_rate * beta_hat_grad
  }
  losses_list[[as.character(learning_rate)]] <- data.frame(epoch = 1:epochs, loss = losses, learning_rate = as.factor(learning_rate))
}

# 손실 데이터를 데이터프레임으로 변환
losses_data <- do.call(rbind, losses_list)

# 각 학습률에 따른 손실 변화를 시각화
p1 <- ggplot(subset(losses_data, learning_rate == '0.001'), aes(x = epoch, y = loss)) +
  geom_line() +
  labs(title = "Learning rate: 0.001", x = "epoch", y = "MSE") +
  theme_minimal()

p2 <- ggplot(subset(losses_data, learning_rate == '1e-04'), aes(x = epoch, y = loss)) +
  geom_line() +
  labs(title = "Learning rate: 1e-04", x = "epoch", y = "MSE") +
  theme_minimal()

p3 <- ggplot(subset(losses_data, learning_rate == '1e-05'), aes(x = epoch, y = loss)) +
  geom_line() +
  labs(title = "Learning rate: 1e-05", x = "epoch", y = "MSE") +
  theme_minimal()

# 그래프들을 하나의 레이아웃에 배치
grid.arrange(p1, p2, p3, ncol = 3)
```

```{r}
# 선택한 변수들
X_cols <- c("seeds", "fruitset", "fruitmass")

# 입력 변수를 표준화하고 상수항을 추가합니다.
df_berry_std_const <- df_berry %>%
  mutate(across(all_of(X_cols), ~ (. - mean(.)) / sd(.))) %>%
  mutate(const = 1) %>%
  select(all_of(X_cols), const) %>%
  as.matrix()

# 앞의 5개 행을 출력합니다.
head(df_berry_std_const, 5)
```

```{r}
# 학습률과 에포크 설정
epochs <- 50
learning_rates <- c(0.1, 0.01, 0.001)
n <- nrow(df_berry_std_const)

# 각 학습률에 따른 손실을 저장할 리스트 초기화
losses_list <- list()

for (learning_rate in learning_rates) {
  losses <- numeric(epochs)
  set.seed(123)
  beta_hat <- runif(length(X_cols) + 1, min = -1, max = 1)
  for (i in 1:epochs) {
    residuals <- df_berry$yield - as.vector(as.matrix(df_berry_std_const) %*% beta_hat)
    losses[i] <- mean(residuals^2)
    beta_hat_grad <- -2 * (t(as.matrix(df_berry_std_const)) %*% residuals) / n
    beta_hat <- beta_hat - learning_rate * beta_hat_grad
  }
  losses_list[[as.character(learning_rate)]] <- data.frame(epoch = 1:epochs, loss = losses, learning_rate = as.factor(learning_rate))
}

# 손실 데이터를 데이터프레임으로 변환
losses_data <- do.call(rbind, losses_list)

# 각 학습률에 따른 손실 변화를 시각화
p1 <- ggplot(subset(losses_data, learning_rate == '0.1'), aes(x = epoch, y = loss)) +
  geom_line() +
  labs(title = "Learning rate: 0.1", x = "epoch", y = "MSE") +
  theme_minimal()

p2 <- ggplot(subset(losses_data, learning_rate == '0.01'), aes(x = epoch, y = loss)) +
  geom_line() +
  labs(title = "Learning rate: 0.01", x = "epoch", y = "MSE") +
  theme_minimal()

p3 <- ggplot(subset(losses_data, learning_rate == '0.001'), aes(x = epoch, y = loss)) +
  geom_line() +
  labs(title = "Learning rate: 0.001", x = "epoch", y = "MSE") +
  theme_minimal()

# 그래프들을 하나의 레이아웃에 배치
grid.arrange(p1, p2, p3, ncol = 3)
```

### 확률적 경사 하강법(Stochastic  Gradient Descent)


학습 데이터가 고르게 분포한다면, 일부분 데이터의 경사도와 전체 데이터의 경사도가 유사합니다. 

$\frac{dL_{batch}}{d\hat{\beta}} \approx \frac{dL}{d\hat{\beta}}$

1 epoch에서 전체 데이터셋을 일부분씩 나누어 경사를 구하고 업데이트하여, 경사도 계산 시간과 메모리를 줄여 효율적으로 경사하강법을 수행합니다.


- 전체에서 경사(Gradient)를 구할 때의 결과는 늘 같지만, 나누어서 경사를 구할 때는 어떻게 셋이 나뉘느냐에 따라서 결과가 달라지게 되어 확률적(Stochastic) 경사 하강법이라고 합니다. 


- 배치(Batch): 전체의 데이터셋을 작은 단위로 나뉘어진 데이터셋들을 말합니다

> 이 배치(Batch)의 크기가 1인 경우를 Stochastic Gradient Descent라고 1 보다 클 경우를 (Batch Gradient Descent)라고 구분하기도 합니다.

- 일부분의 데이터를 가지고 학습이 이루어지므로 굳이 모든 데이터를 메모리에 올릴 필요없이 배치를 만드는데 필요한 만큼만 메모리에 올리면 되므로 메모리 효율적이고, 대용량의 데이터를 학습할 수 있습니다.


- 일부의 데이터로 파라메터의 업데이트를 하므로 수렴이 상대적으로 적은 수의 반복으로 빠르게 될 가능성이 높습니다.


- 데이터의 분포가 고르지 않고 편중되어 있다면 제대로 학습이 되기 어렵습니다. 편중되지 않도록 데이터를 잘 섞어주어야 합니다.

**[Ex.4]** 

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px;">

[Ex. 3]에서 배치(batch)의 크기를 32로 하여 배치(Batch) 경사하강법으로 학습을 합니다.
    
</div>

```{r}
# 파라미터 초기화
set.seed(123)
beta_hat <- runif(length(X_cols) + 1, min = -1, max = 1)
beta_hat

# 학습률과 에포크 설정
epochs <- 50
batch_size <- 32
learning_rate <- 5e-2  # 파라미터 업데이트 비율입니다.
n <- nrow(df_berry)
losses <- numeric(epochs)

# 미니배치 경사 하강법 적용
for (i in 1:epochs) {
  for (j in seq(1, n, by = batch_size)) {
    X_batch <- df_berry_std_const[j:min(j+batch_size-1, n), ]
    y_batch <- df_berry$yield[j:min(j+batch_size-1, n)]
    residuals <- y_batch - as.vector(X_batch %*% beta_hat)
    beta_hat_grad <- -2 * (t(X_batch) %*% residuals) / nrow(X_batch)
    beta_hat <- beta_hat - learning_rate * beta_hat_grad
  }
  losses[i] <- mean((df_berry$yield - as.vector(df_berry_std_const %*% beta_hat))^2)
}

# 결과 출력
print(beta_hat)
print(losses)
```

```{r}
# 손실 데이터를 데이터프레임으로 변환
losses_data <- data.frame(epoch = 1:epochs, loss = losses)

# 손실을 시각화
ggplot(losses_data, aes(x = epoch, y = loss)) +
  geom_line() +
  labs(x = "Epoch", y = "Loss (MSE)") +
  theme_minimal() +
  ggtitle("Loss per Epoch")
```

### 조기 종료(Early Stopping)

학습셋의 일부를 분리하여 매 Epoch 마다 성능을 측정하여, 이 때 성능이 개선이 없으면 학습을 종료하도록 합니다.

이를 통해 불필요한 학습 과정을 생략하고, 과도하게 학습 데이터에 맞춰지지 않도록 합니다.


**[Ex.5]** 

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px;">

[Ex. 4]에서 학습 데이터의 10% 만큼을 조기 종료 데이터로 만들어 조기 종료 루틴이 포함된 경사 하강법을 수행합니다.
    
</div>

```{r}
library(caret)

# 데이터 분할
set.seed(123)
train_index <- createDataPartition(df_berry$yield, p = 0.9, list = FALSE)
X_berry_std_fit <- df_berry_std_const[train_index, ]
X_berry_std_es <- df_berry_std_const[-train_index, ]
y_berry_fit <- df_berry$yield[train_index]
y_berry_es <- df_berry$yield[-train_index]

# 파라미터 초기화
set.seed(123)
beta_hat <- runif(length(X_cols) + 1, min = -1, max = 1)
beta_hat

# 학습률과 에포크 설정
epochs <- 1000
batch_size <- 32
learning_rate <- 1e-2  # 파라미터 업데이트 비율입니다.
n <- nrow(X_berry_std_fit)
losses <- numeric()
prev_loss <- Inf

# 미니배치 경사 하강법 적용 및 조기 종료
for (i in 1:epochs) {
  for (j in seq(1, n, by = batch_size)) {
    X_batch <- X_berry_std_fit[j:min(j+batch_size-1, n), ]
    y_batch <- y_berry_fit[j:min(j+batch_size-1, n)]
    residuals <- y_batch - as.vector(X_batch %*% beta_hat)
    beta_hat_grad <- -2 * (t(X_batch) %*% residuals) / nrow(X_batch)
    beta_hat <- beta_hat - learning_rate * beta_hat_grad
  }
  es_loss <- mean((y_berry_es - as.vector(X_berry_std_es %*% beta_hat))^2)
  if (prev_loss < es_loss) {
    cat('Early Stopped: ', i, prev_loss, es_loss, '\n')
    break
  }
  prev_loss <- es_loss
  losses <- c(losses, mean((y_berry_fit - as.vector(X_berry_std_fit %*% beta_hat))^2))
}

# 결과 출력
print(beta_hat)
print(losses)

# 손실을 시각화
losses_data <- data.frame(epoch = 1:length(losses), loss = losses)

ggplot(losses_data, aes(x = epoch, y = loss)) +
  geom_line() +
  labs(x = "Epoch", y = "Loss (MSE)") +
  theme_minimal() +
  ggtitle("Loss per Epoch")
```

**학습률 스캐쥴링**

학습률을 진행상황에 따라 조정합니다. 학습이 진행이 될 수록 학습률을 줄여 보다 안정적으로 파라메터를 수렴시키도록 하고, 또한 보다 세밀하게 파라메터의 최적공간을 탐색하도록합니다.

지수 감소(Exponential Decay): $\text{learing_rate(학습률)} = \text{Initial learning rate(초기 학습률)}×\text{감소율}^{t}$

$t$: epoch 수

**[Ex.6]** 

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px;">

[Ex. 4]에서 학습률 스캐쥴링을 추가하고, 학습율 감소율을 0.98로 설정하여 학습을 해봅니다.
    
</div>

```{r}
# 파라미터 초기화
set.seed(123)
beta_hat <- runif(length(X_cols) + 1, min = -1, max = 1)
beta_hat

# 학습률과 에포크 설정
epochs <- 50
batch_size <- 32
learning_rate_init <- 1e-2  # 초기 학습률
rate <- 0.95
n <- nrow(X_berry_std_fit)
losses <- numeric()
learning_rates <- numeric()
prev_loss <- Inf

# 미니배치 경사 하강법 적용 및 조기 종료
for (i in 1:epochs) {
  learning_rate <- learning_rate_init * rate^i
  learning_rates <- c(learning_rates, learning_rate)
  for (j in seq(1, n, by = batch_size)) {
    X_batch <- X_berry_std_fit[j:min(j+batch_size-1, n), ]
    y_batch <- y_berry_fit[j:min(j+batch_size-1, n)]
    residuals <- y_batch - as.vector(X_batch %*% beta_hat)
    beta_hat_grad <- -2 * (t(X_batch) %*% residuals) / nrow(X_batch)
    beta_hat <- beta_hat - learning_rate * beta_hat_grad
  }
  es_loss <- mean((y_berry_es - as.vector(X_berry_std_es %*% beta_hat))^2)
  if (prev_loss < es_loss) {
    cat('Early Stopped: ', i, prev_loss, es_loss, '\n')
    break
  }
  prev_loss <- es_loss
  losses <- c(losses, mean((y_berry_fit - as.vector(X_berry_std_fit %*% beta_hat))^2))
}

# 결과 출력
print(beta_hat)
print(losses)

# 손실 데이터프레임 생성
losses_data <- data.frame(epoch = 1:length(losses), loss = losses)

# 학습률 데이터프레임 생성
learning_rates_data <- data.frame(epoch = 1:length(learning_rates), learning_rate = learning_rates)

# 손실을 시각화하는 플롯 생성
p1 <- ggplot(losses_data, aes(x = epoch, y = loss)) +
  geom_line() +
  labs(x = "Epoch", y = "MSE") +
  theme_minimal() +
  ggtitle("Loss")

# 학습률을 시각화하는 플롯 생성
p2 <- ggplot(learning_rates_data, aes(x = epoch, y = learning_rate)) +
  geom_line() +
  labs(x = "Epoch", y = "Learning Rate") +
  theme_minimal() +
  ggtitle("Learning Rate")

# 두 개의 플롯을 하나의 레이아웃에 배치
grid.arrange(p1, p2, ncol = 2)
```
## 2. EM 알고리즘(Expectation Maximization)

EM 알고리즘은 미지의 변수(latent variables)를 포함하고 있는 통계 모델의 모수를 구하는 알고리즘입니다.

### 상세 배경

1. 확률 모델은 $X$: 관측 데이터, $Z$: 미지의 변수, $\theta$: 모수 벡터와 우도 함수 $L(\theta; X, Z) = p(X, Z|\theta)$ 로 구성됩니다. 


2. 최대우도추정를 통한 모수의 추정은 관측데이터의 주변 우도를 최대화하는 수를 모수로 추정하는 것입니다.


1. 2를 공식화하면 아래와 같습니다.

<div style="border: 0px; margin-top: 10px; margin-left: 20px">

$\theta=argmax_\theta L(\theta;X)$ 

$L(\theta;X) = p(X|\theta) = \int p(X,Z|\theta)dZ = \int p(X|Z,\theta)p(Z|\theta)dZ$

</div>
    
하지만, $Z$는 미지의 변수이기 때문에, $\theta$를 알기전에는 $Z$의 분포를 알 수 없습니다. 

아래의 코드는 두 개의 정규 분포에서 생성한 표본입니다.

$N(-10, 9)$ 에서 300개의 표본, $N(10, 4)$에서 550개

이 표본들의 모수를 모르고 또한 표본들이 속한 모집단을 모르는 상태에서

두 모집단의 모수를 추정해야 하는 상황을 생각해봅니다.

```{r}
# 정규분포로부터 샘플 생성
set.seed(123)
x1 <- rnorm(300, mean = -10, sd = 3)
x2 <- rnorm(550, mean = 10, sd = 2)
X <- c(x1, x2)

# 데이터를 데이터프레임으로 변환
df_X <- data.frame(X = X)

# 히스토그램 시각화
ggplot(df_X, aes(x = X)) +
  geom_histogram(bins = 30, fill = "blue", color = "black", alpha = 0.7) +
  labs(x = "X", y = "Count") +
  theme_minimal() +
  ggtitle("Histogram of Combined Normal Distributions")
```
x는 두 개의 정규 분포가 섞여 있는 분포 입니다. 

두 개의 정규 분포의 모수인 평균과 분산을 추정하기 위해서는

각 표본들이 어떤 분포에 속한지 알아야 합니다. 

하지만 모수를 알 지 못한다면, 표본이 어떤 분포에 속하는지를 알 수 없는 상황이 됩니다.

그렇기 때문에 평균과 분산을 추정하기도 어려워집니다.

### Expectation Maximization(EM) 알고리즘

#### 아이디어

일단, $\theta$ 임의의 수로 정의한후 Z(미지의 변수, latent variables)를 구합니다.

$Z$와 $X$를 가장 잘 설명하는(최대 우도를 갖는) $\theta$를 구하고, 이 $\theta$로 다시 Z를 구하는 일련의 과정을 반복합니다.

#### 알고리즘

1. $\theta$를 초기화 합니다.


2. 기대값 단계(Expectation, E step) 

<div style="border: 0px; margin-top: 10px; margin-left: 20px">

$Q(\theta'|\theta)$: 현재 모수 $\theta$로 정의된 잠재 변수의 확률 분포에서 다음 단계 모수 $\theta'$에 대한  로그 우도 $log(p(X, Z| \theta'))$에 대한 기대값입니다.

> $Q(\theta'|\theta) = E_{Z \sim p( \cdot|X, \theta)}\left[log P(X, Z | \theta') \right]$ 
    
</div>

3. 최대화 단계(Maximization, M step)

<div style="border: 0px; margin-top: 10px; margin-left: 20px">

$\theta' = argmax_{\theta'} Q(\theta' | \theta)$

</div>
    
4. $\theta = \theta'$ 로 업데이트하고 수렴할 때까지 2부터 반복합니다. 

**[Ex.7]** 

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px;">

위에서 생성한 표본을 생성한 분포의 모수를 EM 알고리즘으로 구하기 위해 $Q(\theta'|\theta)$를 최대화하는 $\theta'$을 구해봅니다.
    
</div>

$Q(\theta'|\theta)$을 구하기 위해 먼저 $Z \sim p(\cdot | X, \theta)$ 를 구해 봅니다.

**기대값(Expectation) 단계**

$T_{j,i} = P(Z_i = j | X_i = x_i; \theta)$

<div style="border: 0px; margin-top: 10px; margin-left: 20px">

$T_{j,i} = \frac{P(X_i = x_i | Z_i = j; \theta) P(Z=j)}{P(X_i=x_i; \theta)}$

$T_{j,i} = \frac{P(X_i = x_i | Z_i = j; \theta) P(Z=j)}{\sum_{k = 1}^CP(X_i = x_i | Z_i = k; \theta) P(Z = k)}$

> $X_i | Z_i = 1 \sim N(\mu_1, \sigma_1^2)$, ... , $X_i | Z_i = C \sim N(\mu_c, \sigma_c^2)$
>
> $P(Z = 1) = \tau_1$, ... ,$P(Z = C) = \tau_c$
    
</div>

X의 분포와 Z의 분포를 정의하기 위해 필요한 모수는  $\theta = (\mu, \sigma, \tau)$ 입니다.

$Q(\theta'|\theta) = \sum_{i=1}^n \sum_{j=1}^C T_{ji} \left( logP(X_i=x_i, Z_i = j| \theta') \right)$

<div style="border: 0px; margin-top: 10px; margin-left: 20px">

$logP(X_i=x_i, Z_i = j| \theta') = log(P(X_i=x_i|Z_i = j; \theta) + log P(Z=j; \theta')$

$ = log(\frac{1}{\sqrt{2\pi}\sigma_j}) - \frac{1}{2}\left(\frac{x_i - \mu_j}{\sigma_j}\right)^2 + log \tau_j$

</div>

**최대화(Maximization) 단계**

$Q(\theta'|\theta) = \sum_{i=1}^n \sum_{j=1}^C T_{j,i} \left( log(\frac{1}{\sqrt{2\pi}\sigma_j}) - \frac{1}{2}\left(\frac{x_i - \mu_j}{\sigma_j}\right)^2 + log \tau_j \right)$

$\tau' = argmax_\tau \sum_{i=1}^n \sum_{j=1}^C T_{j,i}log\tau_j$

> $\tau'_j = \frac{1}{n}\sum_{i=1}^n T_{j,i}$
    
$(\mu', \sigma') = argmax_{\mu, \sigma} \sum_{i=1}^n \sum_{j=1}^C T_{j,i} \left( -log(\sigma_j) - \frac{1}{2}\left(\frac{x_i - \mu_j}{\sigma_j}\right)^2 \right)$

>$\mu'_j = \frac{\sum_{i=1}^nT_{j,i}x_i}{\sum_{i=1}^n T_{j,i}}$, 
>
>$\sigma'^2_j = \frac{\sum_{i=1}^nT_{j,i}(x_i - \mu_j)^2}{\sum_{i=1}^n T_{j,i}}$
    
**[Ex.8]** 

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px;">

[Ex.7]에서 도출한 내용들로 두 개의 정규 분포의 모수를 EM 알고리즘을 통해 점추정해봅니다.
    
</div>

```{r}
# 데이터 샘플 생성 (위에서 생성된 X 데이터 사용)
set.seed(123)
x1 <- rnorm(300, mean = -10, sd = 3)
x2 <- rnorm(550, mean = 10, sd = 2)
X <- c(x1, x2)

# 모수를 초기화합니다.
n_mixture <- 2
n <- length(X)

set.seed(123)
mu <- runif(n_mixture, min = -5, max = 5)
mu <- mu + mean(X)

sigma <- rep(1, n_mixture)
tau <- rep(1 / n_mixture, n_mixture)

# 출력하여 확인합니다.
mu
sigma
tau
```
```{r}
# Expectation 단계
# T_ij: 각 데이터 포인트가 속하는 분포의 확률을 구합니다.
T <- sapply(1:n_mixture, function(i) dnorm(X, mean = mu[i], sd = sigma[i]) * tau[i])
T <- T / rowSums(T)

# T 확인
head(T)
```
```{r}
# Maximiztion 단계 1: tau
T_sum <- colSums(T)
tau_prime <- T_sum / n

# tau_prime 확인
tau_prime
```
```{r}
# Maxmization 단계 2: mu, sigma
mu_prime <- sapply(1:n_mixture, function(i) sum(T[, i] * X) / T_sum[i])
sigma_prime <- sapply(1:n_mixture, function(i) sqrt(sum(T[, i] * (X - mu_prime[i])^2 / T_sum[i])))

# mu_prime과 sigma_prime 확인
mu_prime
sigma_prime
```
**[Ex.9]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px;">

[Ex.8]를 기반으로 EM 알고리즘의 업데이트 과정을 initiial, 3, 6, 9 번 했을 때의 학습 진행 과정을 시각화해봅니다.
    
</div>


```{r}
# 모수를 초기화합니다.
n_mixture <- 2
n <- length(X)

set.seed(123)
mu <- runif(n_mixture, min = -5, max = 5)
mu <- mu + mean(X)
sigma <- rep(1, n_mixture)
tau <- rep(1 / n_mixture, n_mixture)

print(mu)
print(sigma)
# EM 알고리즘 실행
cnt <- 2
for (it in 1:10) {
  # Expectation 단계
  T <- sapply(1:n_mixture, function(i) dnorm(X, mean = mu[i], sd = sigma[i]) * tau[i])
  T <- T / rowSums(T)
  
  # Maximiztion 단계 1: tau
  T_sum <- colSums(T)
  tau_prime <- T_sum / n

  # Maxmization 단계 2: mu, sigma
  mu_prime <- sapply(1:n_mixture, function(i) sum(T[, i] * X) / T_sum[i])
  sigma_prime <- sapply(1:n_mixture, function(i) sqrt(sum(T[, i] * (X - mu_prime[i])^2 / T_sum[i])) )

  tau <- tau_prime
  mu <- mu_prime
  sigma <- sigma_prime
}

```

## 3. MCMC(Markov Chain Monte Carlo)

MCMC는 확률적 표본추출 기법 중의 하나로, 목표 분포에서 표본을 추출합니다. 

마르코프 체인에서의 이전 상태와의 독립성과 Monte Carlo의 무작위 시도를 통한 산출 방법을 응용하여 복잡한 확률 분포의 표본을 추출하는 방법들을 포괄적으로 일컫습니다.

통계와 머신러닝에서는 모수를 샘플링하여 모수를 추정에 사용합니다.

MCMC의 알고리즘의 한 예로 메트로폴리스-헤이스팅스 알고리즘(Metropolis-Hastings) 이 있습니다.

메트로폴리스-헤이스팅스 알고리즘은 주어진 모수의 사전(Prior) 분포에서 관측 데이터에 대한 모수의 사후(Posterior)의 분포에서 모수를 추출하여, 사후 분포 상의 모수를 추정하는 것에 사용합니다.


### 몬테 카를로 측정(Monte Carlo Estimation)

- 무작위 시도의 반복을 통하여 대상값을 도출합니다.


- 특히 해석적으로 풀이가 거의 불가능한 문제의 값을 구합니다.


**[Ex.10]** 

반지름 1인 원의 넓이를 Monte Carlo 방법을 통해 구해봅니다.

반지름이 1인 원의 넓이: $\pi = 3.141592653589793 ... $

2×2 정사각형에 임의의 점을 찍을 때 원 안에 위치할 확률을 p라고 하면

$\text{원의 넓이} \approx p \cdot 4\text{(2×2 정사각형 넓이)}$

```{R}
set.seed(123)

# 시도 횟수
n_trial <- 10000

# -1과 1 사이의 균일 분포에서 난수를 생성합니다.
x <- runif(n_trial, min = -1, max = 1)
y <- runif(n_trial, min = -1, max = 1)

# 원 안에 위치할 확률을 계산합니다.
is_in_circle <- sqrt(x^2 + y^2) <= 1

# 원주율(π) 추정
p <- sum(is_in_circle) / n_trial
pi_estimate <- p * 4

# 결과 출력
pi_estimate
```
### 마르코프 체인(Markov Chain)

시간이나 공간의 흐름에 따라 특정 상태(state)로의 전이를 모델링하는 수학적 도구입니다. 이는 확률적인 시스템의 행동을 분석하고 예측하는 곳에 사용됩니다.

시점(또는 지점)마다 상태를 가지고 있고, 각 상태는 일정한 확률로 전이될 수 있습니다.

- 상태 공간(State Space): 시스템이 취할 수 있는 가능한 모든 상태들을 나타냅니다.


- 전이 확률(Transition Probability): 한 상태에서 다른 상태로 전이될 확률을 나타냅니다. 

> 전이 확률 행렬: 전이 확률을 요소로 담고 있는 행렬입니다.
>
> $P = \begin{bmatrix}
P_{11} & P_{12} & \cdots & P_{1n} \\
P_{21} & P_{22} & \cdots & P_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
P_{n1} & P_{n2} & \cdots & P_{nn}
\end{bmatrix}$,  $P_{ij}$: i 상태에서 j 상태로 전이될 확률입니다.
>
> 임의의 j에 대해서 $\sum_{i=1}^n P_{ji} = 1$ 입니다.
>
> 상태 확률을 행벡터 v라고 하면, 다음 상태 확률을 v'라고 하면 $v' = v P$

- 마르코프 속성(Markov Property): 다음 상태로 전이는 오직 현재 상태하고만 연관이 있습니다. 메모리리스(memoryless)

**[Ex.11]**

상태 공간은 {E, A} 입니다. 아래 다이어그램은 E와 A사이의 전이 확률을 나타냅니다.

현재 E 상태일 확률은 0.7, A 상태일 확률이 0.3 일 때, 1에서 20단계가 지나후의 E 상태일 때와 A 상태일 확률을 구하여 그래프로 출력해봅니다.


![image.png](data:image/png;base64,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)

※ 이미지 출처: https://en.wikipedia.org/wiki/Markov_chain

```{r}
library(tidyr)

# 초기 확률입니다.
v <- c(0.7, 0.3)

# 전이 확률 행렬입니다.
P <- matrix(c(0.3, 0.7, 0.4, 0.6), nrow = 2, byrow = TRUE)

# 초기 확률을 리스트로 저장합니다.
v_list <- list(v)

# 20번 반복하여 확률을 계산합니다.
for (i in 1:20) {
  v_list[[i + 1]] <- v_list[[i]] %*% P
}

# 리스트를 행렬로 변환합니다.
v_matrix <- do.call(rbind, v_list)

# 데이터 프레임으로 변환하여 시각화를 준비합니다.
v_df <- data.frame(Iteration = 0:20, E_probability = v_matrix[, 1], A_probability = v_matrix[, 2])
v_df_long <- pivot_longer(v_df, cols = c(E_probability, A_probability), names_to = "State", values_to = "Probability")

# 확률을 시각화합니다.
ggplot(v_df_long, aes(x = Iteration, y = Probability, color = State)) +
  geom_line() +
  labs(title = "Markov Chain Probabilities", x = "Iteration", y = "Probability") +
  theme_minimal() +
  theme(legend.title = element_blank())
```

**[Ex. 12]**

상태 공간은 {A, B} 입니다. A에서 B로 이동할 확률은 1.0이고, B에서 A로 이동할 확률은 1.0입니다.

현재 A 상태일 확률은 0.7, B 상태일 확률이 0.3 일 때, 1에서 20단계가 지나후의 A 상태일 때와 B 상태일 확률을 구하여 그래프로 출력해봅니다.

```{r}
# 초기 확률입니다.
v <- c(0.7, 0.3)

# 전이 확률 행렬입니다.
P <- matrix(c(0.0, 1.0, 1.0, 0.0), nrow = 2, byrow = TRUE)

# 초기 확률을 리스트로 저장합니다.
v_list <- list(v)

# 20번 반복하여 확률을 계산합니다.
for (i in 1:20) {
  v_list[[i + 1]] <- v_list[[i]] %*% P
}

# 리스트를 행렬로 변환합니다.
v_matrix <- do.call(rbind, v_list)

# 데이터 프레임으로 변환하여 시각화를 준비합니다.
v_df <- data.frame(Iteration = 0:20, E_probability = v_matrix[, 1], A_probability = v_matrix[, 2])
v_df_long <- pivot_longer(v_df, cols = c(E_probability, A_probability), names_to = "State", values_to = "Probability")

# 확률을 시각화합니다.
ggplot(v_df_long, aes(x = Iteration, y = Probability, color = State)) +
  geom_line() +
  labs(title = "Markov Chain Probabilities", x = "Iteration", y = "Probability") +
  theme_minimal() +
  theme(legend.title = element_blank())
```

### 메트로폴리스-헤이스팅스(Metropolis-Hastings) 알고리즘

MCMC 알고리즘에서 가장 기본적이면서 널리 사용되는 알고리즘 중 하나입니다.

목표 분포 $f(\theta)$로 부터 표본을 추출합니다.


#### 알고리즘

i번째의 표본을 $\theta_i$라 정의합니다.

1. 시작 상태 설정: 상태 공간에 임의의 상태로 시작 합니다.($\theta_0$ 추출)


2. 표본 제안(Sample Proposal): 현재 상태에서 새로운 상태를 제안합니다 $\theta_{t + 1}$를 제안 받습니다.


3. 수용 확률(Acceptance Probability): 제안한 상태($\theta_{t + 1}$}와 현재 상태($\theta_{t}$)의 목표 분포를 구합니다. 

> 다음 표본이 수용될 확률을 현재 상태만을 고려하여 구한다는 점이 Markov Chain을 이용한 점입니다.
>
> $\alpha = \frac{f(\theta_{t + 1})}{f(\theta_t)}$
>
> $\alpha \ge 1$ 이면 무조건 수용
>
> $\alpha \lt 1$ 이면 몬테 카를로 기법을 응용하여 [0, 1] 사이의 난수를 생성하여 이 숫자가 $\alpha$ 보다 작으면 수용

4. 현재 수용된 표본을 취하고, 목표한 표본을 얻을 때까지 2를 반복합니다.

####  모수의 사후 분포에서 모수 추출 

<div style="border: 0px solid #ddd; padding: 12px; margin-top: 10px;">

모수의 사전(Prior) 분포와 관측 데이터에서 모수의 사후(Posterior) 분포에서 모수를 추출합니다.

즉, 사전에 알고 있는 모수에 대한 지식을 바탕으로 관측 데이터를 통해 얻어지는 모수의 분포를 추정해봅니다.

베이즈 규칙을 이용하여 사전 분포와 사후 분포의 관계를 식으로 나타내면 아래와 같습니다.

> $p(\theta' | X, \theta) = \frac{P(X=x_{1}, ... X=x_{n}|\theta) P(\theta)}{P(X=x_{1}, ... X=x_{n})}$


$P(X=x_{1}, ... X=x_{n})$를 구하는 것은 굉장히 어려운 일이기 때문에, 모수의 사후 확률 분포를 직접 계산하는 것 또한 어렵습니다.

메트로폴리스-헤이스팅스를 적용하면, 모수의 사후 분포 모수를 추출할 수 있습니다.


아래 메트로폴리스-헤이스팅스의 수용확률에 사후 확률 분포를 적용을 하면,

> $\alpha = \frac{f(\theta_{t + 1})}{f(\theta_t)}$

이전 상태의 확률이나 이후 제안의 증거 확률은 $P(X=x_{1}, ... X=x_{n})$으로 동일하기 때문에 

> $\alpha = \frac{P(X=x_{1}, ... X=x_{n}|\theta') P(\theta')}{P(X=x_{1}, ... X=x_{n}|\theta) P(\theta)}$

로 나타내어 증거확률을 계산할 필요가 없게 됩니다.


이러한 원리로 사후 확률 계산에서의 난제인 증거 확률 계산 문제를 해소할 수 있습니다.
    
</div>

**[Ex.13]** 

아래  표본들은 정규분포에서 추출된 표본입니다. 이 표본들을 X라고 합니다. 각 표본들은 독립입니다

```
X = [2.83, 6.99, 5.57, 1.99, 3.84, 8.3 , 0.15, 4.14, 7.53, 3.27, 
    3.64, 4.81, 7.98, 3.72, 4.11, 4.13, 9.41, 9.37, 7.01, 5.77]
```

이 정규분포의 평균 ($\mu$)의 사전(Prior) 분포는 N(3, 4)이고 분산($\sigma^2$)의 사전 분포는 N(5, 2)입니다.


목표 분포 함수는 아래와 같이 유도됩니다.

> $p(\mu', \sigma' | X, \mu, \sigma) \propto P(X=x_{1}, ... X=x_{n}|\mu, \sigma) P(\mu, \sigma) = \prod_{i=1}^{n}  P(X=x_i|\mu, \sigma) P(\mu, \sigma)$


사후 확률 분포의 모수 $\mu', \sigma'$를 추정하기 위해,

$\mu', \sigma'$을 메트로폴리스-헤이스팅스 알고리즘을 사용하여 5000개 추출해봅니다.

표본의 제안 방법은 아래와 같습니다.

> $\mu_{t+1} \sim N(\mu_t, 1)$
>
> $\sigma_{t+1} \sim Chi^2(1) × (\sigma_{t})$


수집한 10000개에서 표본 분포의 수렴점과 자기 상관성을 시각화를 통해 확인하여, 유효한 표본을 선택해봅니다.

```{r}
set.seed(123)

# 데이터
X <- c(2.83, 6.99, 5.57, 1.99, 3.84, 8.3, 0.15, 4.14, 7.53, 3.27, 
       3.64, 4.81, 7.98, 3.72, 4.11, 4.13, 9.41, 9.37, 7.01, 5.77)
prior_mean_mu <- 3
prior_mean_sigma <- 4
prior_var_mu <- 5
prior_var_sigma <- 2
eps <- 1e-7

# 목표 분포 정의
target_log_distribution <- function(x) {
  log_likelihood <- sum(log(dnorm(X, mean = x[1], sd = x[2]) + eps))
  prior <- log(dnorm(x[1], mean = prior_mean_mu, sd = sqrt(prior_mean_sigma)) + eps) + 
           log(dnorm(x[2], mean = prior_var_mu, sd = sqrt(prior_var_sigma)) + eps)
  log_likelihood + prior
}

# 제안 분포 정의
proposal_distribution <- function(x) {
  c(rnorm(1, mean = x[1], sd = 1), rchisq(1, df = 1) * x[2])
}

# 수용 확률 계산
acceptance_probability <- function(x_new, x_current) {
  min(1, exp(target_log_distribution(x_new) - target_log_distribution(x_current)))
}

# Metropolis-Hastings 알고리즘
metropolis_hastings <- function(iterations) {
  samples <- matrix(NA, nrow = iterations, ncol = 2)
  x_current <- c(0, 1)
  
  for (i in 1:iterations) {
    x_new <- c(rnorm(1, mean = x_current[1], sd = 1), rchisq(1, df = 1) * x_current[2])
    alpha <- acceptance_probability(x_new, x_current)
    u <- runif(1)
    
    if (u < alpha) {
      x_current <- x_new
    }
    
    samples[i, ] <- x_current
  }
  
  as.data.frame(samples)
}

# Metropolis-Hastings 알고리즘 실행
iterations <- 10000
X_sample <- metropolis_hastings(iterations)
colnames(X_sample) <- c("mean", "variance")

# 결과 시각화
p1 <- ggplot(X_sample, aes(x = 1:iterations, y = mean)) +
  geom_line() +
  labs(x = "Iterations", y = "Mean") +
  theme_minimal()

p2 <- ggplot(X_sample, aes(x = 1:iterations, y = variance)) +
  geom_line() +
  labs(x = "Iterations", y = "Variance") +
  theme_minimal()

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

```{r}
library(forecast)

# Metropolis-Hastings 알고리즘 실행 (이전 코드 사용)
iterations <- 10000
X_sample <- metropolis_hastings(iterations)
colnames(X_sample) <- c("mean", "variance")

# 자기상관 계산 및 시각화
acf_mean <- acf(X_sample$mean, plot = FALSE, lag.max = 100)
acf_variance <- acf(X_sample$variance, plot = FALSE, lag.max = 100)

# ACF plot for mean
p1 <- ggplot(data = data.frame(lag = acf_mean$lag, acf = acf_mean$acf), aes(x = lag, y = acf)) +
  geom_bar(stat = "identity", position = "identity", width = 0.1) +
  labs(x = "Lag", y = "ACF", title = "Autocorrelation of mu'") +
  theme_minimal()

# ACF plot for variance
p2 <- ggplot(data = data.frame(lag = acf_variance$lag, acf = acf_variance$acf), aes(x = lag, y = acf)) +
  geom_bar(stat = "identity", position = "identity", width = 0.1) +
  labs(x = "Lag", y = "ACF", title = "Autocorrelation of sigma'") +
  theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

```{r}
# Metropolis-Hastings 알고리즘 실행 (이전 코드 사용)
iterations <- 10000
X_sample <- metropolis_hastings(iterations)
colnames(X_sample) <- c("mean", "variance")

# 샘플을 선택하여 추정된 평균과 표준편차를 계산합니다.
X_samples_selected <- X_sample[seq(300, nrow(X_sample), by = 50), ]
estimated_mean <- mean(X_samples_selected$mean)
estimated_std <- mean(X_samples_selected$variance)

cat("Estimated Mean:", estimated_mean, "\n")
cat("Estimated Standard Deviation:", estimated_std, "\n")

# 결과 시각화
p1 <- ggplot(X_samples_selected, aes(x = mean)) +
  geom_histogram(bins = 20, fill = "blue", alpha = 0.7) +
  geom_vline(aes(xintercept = estimated_mean), color = "red") +
  labs(x = "Mean", y = "Frequency") +
  theme_minimal()

p2 <- ggplot(X_samples_selected, aes(x = variance)) +
  geom_histogram(bins = 20, fill = "blue", alpha = 0.7) +
  geom_vline(aes(xintercept = estimated_std), color = "red") +
  labs(x = "Variance", y = "Frequency") +
  theme_minimal()

gridExtra::grid.arrange(p1, p2, ncol = 2)
```










