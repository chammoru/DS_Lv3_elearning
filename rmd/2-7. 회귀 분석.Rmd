---
title: "2-7. 회귀 분석"
output: html_notebook
---

2-7. 회귀 분석

## 0. 데이터셋 소개

### Wild blueberry

[Wild blueberry](https://data.mendeley.com/datasets/p5hvjzsvn8/1) 야생 블루베리의 수확량을 시뮬레이션한 데이터셋입니다.


|Features|Unit|Description|
|:--------|:----|:-----------|
|Row#||Index|
|clonesize|m2|The average blueberry clone size in the field|
|honeybee|bees/m2/min|Honeybee density in the field|
|bumbles|bees/m2/min|Bumblebee density in the field|
|andrena|bees/m2/min|Andrena bee density in the field|
|osmia|bees/m2/min|Osmia bee density in the field|
|MaxOfUpperTRange|℃|The highest record of the upper band daily air temperature during the bloom season|
|MinOfUpperTRange|℃|The lowest record of the upper band daily air temperature|
|AverageOfUpperTRange|℃|The average of the upper band daily air temperature|
|MaxOfLowerTRange|℃|The highest record of the lower band daily air temperature|
|MinOfLowerTRange|℃|The lowest record of the lower band daily air temperature|
|AverageOfLowerTRange|℃|The average of the lower band daily air temperature|
|RainingDays|Day|The total number of days during the bloom season, each of which has precipitation larger than zero|
|AverageRainingDays|Day|The average of raining days of the entire bloom season|
|fruitset||The amount of fruit per tree|
|fruitmass||The mass of a fruit|
|seeds||The mass of a seed|
|yield||The amount of yield|




```{r}
library(readr)

# CSV 파일 읽기
df_berry <- read_csv('../data/wild_blueberry.csv')

# 데이터 출력
print(head(df_berry))
```

## 1. 선형 회귀(Linear Regression)

### 구성 요소

- 표현법 1: 수열 형태

$\{y_i, x_{i1}, ... , x_{ip}\}_{i=1}^n$

$y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + ...  + \beta_px_{ip} + \epsilon_i = x_i^T\beta + \epsilon_i$

---------------------
- 표현법 2: 벡터 형태

$\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}$, 
$\mathbf{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_1 \\ \vdots \\ \epsilon_n \end{bmatrix}$,
$\mathbf{x_i} = \begin{bmatrix} 1 \\ x_{i1} \\ \vdots \\ x_{ip} \end{bmatrix}$,
$\mathbf{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_p \end{bmatrix}$

$\mathbf{X} = \begin{bmatrix} x_0^T \\ x_1^T \\ \vdots \\ x_n^T \end{bmatrix}$ = 
$\begin{bmatrix} 1 & x_{11} & x_{12} & \ldots & x_{1p} \\ 1 & x_{21} & x_{22} & \ldots & x_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ 1 & x_{n1} & x_{n2} & \ldots & x_{np} \end{bmatrix}$

$y=X\beta + \epsilon$


$y$: 종속 변수(dependent variable), 내생 변수(endogenous variable, 모델 내에서 다른 변수의 영향을 받는 변수), 대상 변수(target variable), 반응 변수

$X$: 독립 변수(independent variable), 외생 변수(exogenous variable, 모델 내에서 설명되지 않는 변수), 예측 변수(predictor variable), 입력 변수(input variable)

$\beta$: 회귀 계수(regression coefficeints) 

$\epsilon$: 오차


### 선형 회귀 모델의 주요 가정

선형성 (Linearity): 종속 변수와 독립 변수 간의 관계가 선형이어야 합니다. 

독립성(independence): 각각의 관측은 서로 독립입니다. 어떤 관측치의 오차가 다른 관측치의 오차에 영향을 미치지 않아야 합니다.

등분산(Homoscedasticity): 오차의 분산은 독립 변수와 상관없이 일정해야 합니다.

정규성 (Normality of Residuals): 오차는 정규분포를 따라야 합니다. 

완전한 다중공선성 부재 (Lack of perfect multicollinearity): 독립 변수 각각을 종속변수로 했을 때 예측 오차가 없이 완전히 설명이 가능한 변수가 존재하지 않아야 합니다.


### 회귀 계수의 추정

**최소 제곱법 (Least Square Method, Ordinary Least Square)**

$S(\beta) = \sum_{i=1}^n\left(y_i - x_i^T\beta\right)^2 = ||y - X\beta||^2$

$\hat{\beta}= argmin_{\beta}S(\beta)$

$||y - X\beta||^2 = (y - X\beta)^T(y - X\beta)$

$\beta$ 의 추정치를 $\hat{\beta}$이라고 합니다.

$\frac{dS(\hat{\beta})}{d\hat{\beta}} = 0$

$\frac{dS(\hat{\beta})}{d\hat{\beta}} = \frac{d(y^Ty - y^TX\hat{\beta} - \hat{\beta}^TX^Ty + \hat{\beta}^TX^TX\hat{\beta})}{d\hat{\beta}}$

> $\hat{\beta}^TX^Ty$는 1×1 행렬이므로 $\hat{\beta}^TX^Ty = (\hat{\beta}^TX^Ty)^T$
>
> $\hat{\beta}^TX^Ty = y^TX\hat{\beta}$

$\frac{dS(\hat{\beta})}{d\hat{\beta}} = \frac{d(y^Ty - 2y^TX\hat{\beta} + \hat{\beta}^TX^TX\hat{\beta})}{d\hat{\beta}}$

> $\frac{dy^TX\hat{\beta}}{d\hat{\beta}} = (y^TX)^T=X^Ty$
>
> $\frac{d\hat{\beta}^TX^TX\hat{\beta}}{d\hat{\beta}} = 2X^TX\hat{\beta}$

$\frac{dS(\hat{\beta})}{d\hat{\beta}} = -2X^Ty + 2X^TX\hat{\beta}$

$-2X^Ty + 2X^TX\hat{\beta} = 0$

$\hat{\beta}=(X^TX)^{-1}X^Ty$

**회귀 계수의 의미**

회귀 계수($\beta_i, i > 0$): 독립 변수 $X_i$가 1 증가 할 때, 종속 변수의 변화량

상수항(intercept, $\beta_0$): 모든 독립 변수가 0일 때 종속 변수의 평균 ← 모든 독립 변수가 0인 상황에 의미를 부여 할 수 있다면 분석의 대상이 되겠지만, 보통 독립 변수가 0 인 상황이 특별한 의미를 가지지 못한 경우가 많으므로 분석에 대상이 되지는 못합니다.


**[Ex.1]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px">

단순 선형 회귀(Simple Linear Regression) - 독립 변수의 수가 하나인 선형 회귀 모델

df_berry에서 

clonesize == 12.5 and honeybee == 0.25 and bumbles == 0.25 and AverageOfUpperTRange == 71.9 and AverageLowerTRange == 50.8 

인 관측 데이터만 선택하여 df_berry2를 만듭니다.

df_berry2에서 yield 는 종속 변수 fruitset은 독립 변수로 회귀 모델을 만들어 봅니다.
    
</div>

```{r}
# 필터링 조건에 맞는 행 선택
df_berry2 <- df_berry %>%
  filter(clonesize == 12.5, honeybee == 0.25, bumbles == 0.25, AverageOfUpperTRange == 71.9, AverageOfLowerTRange == 50.8)

# 결과 출력
print(df_berry2)
```

```{r}
# 산점도 생성
ggplot(df_berry2, aes(x = fruitset, y = yield)) +
  geom_point(size = 3, alpha = 0.5) +
  labs(title = "Fruitset vs Yield", x = "Fruitset", y = "Yield") +
  theme_minimal()
```

```{r}
# X와 y 설정
X <- df_berry2 %>%
  select(fruitset) %>%
  mutate(const = 1)

y <- df_berry2$yield

# 정규 방정식을 이용한 회귀계수 계산
X_matrix <- as.matrix(X)
y_vector <- as.matrix(y)

beta_hat <- solve(t(X_matrix) %*% X_matrix) %*% t(X_matrix) %*% y_vector

# 결과 출력
beta_hat
```
```{r}
# 회귀선 데이터 생성
fruitset_range <- data.frame(fruitset = range(df_berry2$fruitset))
fruitset_range$const <- 1
fruitset_range$yield <- as.matrix(fruitset_range) %*% beta_hat

# 산점도와 회귀선 그리기
ggplot(df_berry2, aes(x = fruitset, y = yield)) +
  geom_point(size = 3, alpha = 0.5) +
  geom_line(data = fruitset_range, aes(y = yield), color = 'orange') +
  labs(title = "Fruitset vs Yield with Regression Line", x = "Fruitset", y = "Yield") +
  theme_minimal()
```

```{r}
# 선형 회귀 모델 적합
lm_model <- lm(yield ~ fruitset, data = df_berry2)

# 결과 요약
summary(lm_model)
```
```{r}
# 잔차 추출
residuals <- resid(lm_model)

# 잔차 히스토그램 그리기
ggplot(data.frame(residuals), aes(x = residuals)) +
  geom_histogram(bins = 10, fill = "blue", alpha = 0.7) +
  labs(title = "Residuals Histogram", x = "Residuals", y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(size = 14, face = "bold", hjust = 0.5))
```

## 2. 적합도(The goodness-of-fit)

$\sum_{i=1}^n{\left(y_i - \bar{y}\right)^2} = \sum_{i=1}^n{\left(\hat{y_i} - \bar{y}\right)^2} + \sum_{i=1}^n{\left(y_i - \hat{y_i}\right)^2}$

SST = SSR + SSE

SST (Sum of squares Total, TSS:Total sum os squares): 전체 변동성 - 평균에서 얼마나 데이터들이 떨어져 있는지는 나타냅니다.

SSR (Sum of Squared Regression, ESS:Explained sum of squares): 모델에 의해 설명된 변동성

SSE(Sum of squares of errors, RSS:Residual sum of squares): 모델에 의해 설명되지 못한 변동성

**증명**

$SST = ||y - \bar{y}1||^2 = ||y - \hat{y} + \hat{y} - \bar{y}1||^2$

$ = ||y - \hat{y}1||^2 + ||\hat{y} - \bar{y}||^2 + 2<y - \hat{y}, \hat{y} - \bar{y}>$

$ = SSE + SSR + 2(\hat{e}^TX)\hat{\beta} - 2\bar{y}\hat{\epsilon}^T1$, $\hat{\epsilon}=y - \hat{y}$

> $(\hat{e}^TX) = (y - \hat{y})^TX = (y - X(X^TX)^{-1}X^Ty)^TX = y^TX - y^TX(X^TX)^{-1}X^TX = 0$

> $\hat{\epsilon}^T1 = \sum_{i=1}^{n}{y_i - \hat{y_i}}=0$


$ = SSE + SSR$

**결정 계수(Coefficient of determination)**

$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$: 전체 변동성에서 회귀식에 의해 설명된 변동성의 비율

**[Ex.2]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px">

위 모델의 SST, SSR, SSE를 구해 봅니다.
    
</div>

```{r}
# X와 y 설정
X <- df_berry2 %>%
  select(fruitset) %>%
  mutate(const = 1)

y <- df_berry2$yield

# 정규 방정식을 이용한 회귀계수 계산
X_matrix <- as.matrix(X)
y_vector <- as.matrix(y)

beta_hat <- solve(t(X_matrix) %*% X_matrix) %*% t(X_matrix) %*% y_vector

# 예측값 계산
s_prd <- as.matrix(df_berry2 %>% select(fruitset) %>% mutate(const = 1)) %*% beta_hat

# SSE, SSR, SST 계산
sse <- sum((y - s_prd) ^ 2)
ssr <- sum((s_prd - mean(y)) ^ 2)
sst <- sum((y - mean(y)) ^ 2)

# 결과 출력
cat("SSE:", sse, "\n")
cat("SSR:", ssr, "\n")
cat("SST:", sst, "\n")
```
```{r}
# R² 값 계산
r2_1 <- ssr / sst
r2_2 <- 1 - sse / sst

# 결과 출력
cat("R² (방법 1):", r2_1, "\n")
cat("R² (방법 2):", r2_2, "\n")
```

```{r}
# SSE (Sum of Squared Residuals) 계산
sse <- sum(resid(lm_model)^2)

# SSR (Sum of Squares Regression) 계산
ssr <- sum((fitted(lm_model) - mean(df_berry2$yield))^2)

# 결과 출력
cat("SSE:", sse, "\n")
cat("SSR:", ssr, "\n")
```

**수정된 결정 계수**

$R^2$는 독립 변수가 종속 변수와 실제로 관련이 없더라도, 독립 변수의 수를 많이 가져갈 수록 높아지는 경향이 있습니다. 

(p개의 모수(회귀 계수 + 절편)를 지닌 선형모델은 적어도 p개의 점은 오차 없이 예측할 수 있습니다.)

따라서 회귀 계수의 모수의 수에 따른 패널티를 가하여 결정계수를 수정한 것이 수정된 결정 계수 입니다. 

$Adjusted R^2=1 - \frac{MSE}{MST} = 1 - \frac{SSE/(n - p)}{SST/(n - 1)}$


```{r}
# SST (Total Sum of Squares) 계산
sst <- sum((df_berry2$yield - mean(df_berry2$yield))^2)

# 모델의 자유도
p <- length(coef(lm_model))  # number of parameters including the intercept
n <- nrow(df_berry2)

# Adjusted R² 계산
adjusted_r2 <- 1 - (sse / (n - p)) / (sst / (n - 1))

# 결과 출력
cat("Adjusted R²:", adjusted_r2, "\n")
```
### 회귀 모델의 유의성 검정

**SSR(모델이 설명한 변동성), SSE(모델이 설명하지 못한 변동성)을 이용한 F-test**

$H_0$: 모든 회귀계수(절편 제외)는 0이다.$(\beta_1 = \beta_2 = ... \beta_k = 0) \equiv MSR <= MSE$, 회귀 모델은 유의미 하지 않다.

$H_1$: 회귀계수(절편 제외)중 적어도 하나는 0이 아니다.$(\beta_1 \ne 0\,or\,\beta_2 \ne 0\,...\,or\,\beta_k \ne 0) \equiv MSR > MSE$, 회귀 모델은 유의미하다.

$MSR = \frac{SSR}{df_R}$, 회귀모델의 모수의 수가 p일 때, 평균에 의해 고정되므로 1개가 차감됩니다. 따라서 p - 1

$MSE = \frac{SSE}{df_E}$, 모수가 p개는 p개의 표본에 대해서는 오차가 0으로 고정이 됩니다. 따라서 MSE p개 만큼의 표본이 고정되므로 자유도는 n - p 가 됩니다.

$F = \frac{MSR}{MSE}\sim F(p - 1, n - p)$

**[Ex.3]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px">

위 모델의 F-test를 수행해봅니다.
    
</div>

```{r}
# F-검정 통계량 계산
df_ssr <- 1
df_sse <- n - 2
F <- (ssr / df_ssr) / (sse / df_sse)

# p-value 계산
pvalue <- pf(F, df_ssr, df_sse, lower.tail = FALSE)

# 결과 출력
cat("SSR 자유도:", df_ssr, "\n")
cat("SSE 자유도:", df_sse, "\n")
cat("검정통계량 (F):", F, "\n")
cat("p-value:", pvalue, "\n")
```
**회귀 계수의 유의성 검정**

$H_0: \beta_i = 0$

$H_1: \beta_i \ne 0$

$H_0$가 맞다면, 

$\mu_{\hat{\beta_i}}$ = 0

$s_{\hat{\beta_i}}=\sqrt{\frac{\frac{1}{n - p}\sum_{i=1}^{n}\hat{\epsilon_i^2}}{\sum_{i=1}^n\left(x_i - \bar{x}\right)^2}}$

$t = \frac{\hat{\beta_i} - \mu_{\hat{\beta_i}}}{s_{\hat{\beta}}}= \frac{\hat{\beta_i}}{s_{\hat{\beta}}} \sim t(n - p)$

**[Ex.4]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px">

위 모델의 회귀계수에 대한 회귀 계수의 유의성 검정을 해봅니다.
    
</div>

```{r}
# 회귀 모델의 자유도와 총 데이터 포인트 수
n <- nrow(df_berry2)
p <- length(coef(lm_model))  # number of parameters including the intercept

# 회귀 계수의 표준 오차 계산
s_beta_hat <- sqrt((sse / (n - p)) / sum((df_berry2$fruitset - mean(df_berry2$fruitset))^2))

# 결과 출력
s_beta_hat
```

```{r}
# 회귀 계수의 표준 오차 계산
s_beta_hat <- sqrt((sse / (n - p)) / sum((df_berry2$fruitset - mean(df_berry2$fruitset))^2))

# 회귀 계수 계산
beta_hat <- coef(lm_model)

# t-검정 통계량 계산
t_stat <- -abs(beta_hat["fruitset"] - 0) / s_beta_hat

# p-value 계산
pvalue <- pt(t_stat, df = n - p) * 2

# 결과 출력
cat("검정통계량 (t):", t_stat, "\n")
cat("p-value:", pvalue, "\n")
```
### AIC (Akaike Information Criterion)

$AIC = 2k - 2 ln(L)$, L 모델의 데이터에 대한 최대우도입니다. k:는 모델의 모수의 수

우도가 클수록 모델이 데이터를 잘 설명하는 것입니다. 

따라서 L을 0과 1사이의 수이므로 AIC가 작을 수록 모델이 데이터를 잘 설명하는 것입니다.

선형 회귀 모델의 AIC

$AIC = 2p + nln(2 \pi) + nln(\frac{SSE}{n}) + n$, n: 표본수, p: 선형 회귀 모델의 모수의 수

상대적 비교 지표이므로, 상수항을 제외하여 나타낼 수도 있습니다.

$AIC = 2p + n ln(\frac{SSE}{n})$

유도 과정입니다.

> $y_i = x_i^T \beta + \epsilon_i$
> 
> $\epsilon_i \sim N(0, \sigma^2)$,  선형 회귀 모델 가정입니다.
>
> 이는 곧, $L(\beta, \sigma^2|y, X) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}}exp(-\frac{(y_i-x_i^T\beta)^2}{2\sigma^2}))$
> 
> 여기에 log 양변에 취하여 로그우도를 취하여 정리합니다.
> 
> $lnL(\beta, \sigma^2|y, X) = -\frac{n}{2}ln(2 \pi \sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n(y_i - x_i^T\beta)^2$
>
> 우도를 최대화하는 \sigma와 \beta는 회귀 계수의 추정 과정으로 도출됩니다. 
>
> $lnL(\beta, \sigma^2|y, X) = -\frac{n}{2}(ln(2 \pi) + ln(\sigma^2)) - \frac{SSE}{2\sigma^2}$
>
> 이 때 $\sigma^2 = \frac{SSE}{n}$ 가 됩니다. 
>
> $lnL(\beta, \sigma^2|y, X) = -\frac{n}{2}(ln(2 \pi) + ln(\frac{SSE}{n})) - \frac{n}{2}$
>
> 이를 AIC 에 대입하면, 이와함께 k를 회귀 계수의 모수의 수 p로 합니다.
>
> $AIC = 2p + nln(2 \pi) + nln(\frac{SSE}{n}) + n$
>
> 성능 비교를 위해 영향을 주지 않는 상수항($n×ln(2 \pi) + n$)을 정리하면 아래와 같이 
>
> $AIC = 2p +  nln(\frac{SSE}{n})$ 이 됩니다.


**[Ex.7]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px">

위 모델의 AIC 를 statsmodels를 기준으로 직접 구해봅니다.

statsmodels의 summary 항목을 봅니다.
    
</div>


```{r}
# 로그 우도 계산
log_likelihood <- (-n / 2 * (log(sse / n) + log(2 * pi))  - n / 2)

# AIC 계산
AIC <- -2 * log_likelihood + 2 * p

# 결과 출력
cat("로그 우도:", log_likelihood, "\n")
cat("AIC:", AIC, "\n")
```
### BIC(Bayesian Information Criterion)

BIC는 AIC보다 변수의 수에 더 많은 페널티를 부여합니다. 

$BIC = pln(n) - 2ln L$ 입니다.


**[Ex.8]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px">

위 모델의 BIC 를 statsmodels를 기준으로 직접 구해봅니다.

statsmodels의 summary 항목을 봅니다.
    
</div>

$BIC = pln(n) + nln(2 \pi) + nln(\frac{SSE}{n}) + n$

```{r}
# 로그 우도 계산
log_likelihood <- (-n / 2 * (log(sse / n) + log(2 * pi))  - n / 2)

# BIC 계산
BIC <- -2 * log_likelihood + log(n) * p

# 결과 출력
cat("로그 우도:", log_likelihood, "\n")
cat("BIC:", BIC, "\n")
```

## 3. 이상점과 영향점

**이상점**: 보편적인 분포에 벋어난 관측치

**영향점**: 이상점 중에서 회귀 분석 결과에 큰 영향을 미치는 소수의 관측치

**이상점 판별법**

1. 시각화


2. 점 하나씩 제외해보고, 회귀계수의 변화의 정도를 측정


3. 통계량

**레버리지(Leverage)**:  i번째 관측치와 평균 관측치의 거리, i번째 종속변수가 i번째 적합값(fitted value)에 미치는 영향도

> $h_{ii} = x_i^T(X^TX)^{-1}x_i$, $X$는 디자인 행렬, $x_i$는 i번째 관측치에 해당하는 벡터
>
> $0 \le h_{ii} \le 1$


**표준화 잔차(studentized residual)**: i번째 관측치의 상대적 잔차

> $t_i = \frac{\hat{e_i}}{\sqrt{MSE(1-h_{ii})}}$

**DFFITs(difference in fit(s))**: i번째 관측치를 제거했을 때 적합값(fitted value)의 변동량

> $DFFITS = \frac{\hat{Y_i} - \hat{Y_{i(i)}}}{\sqrt{MSE_{(i)}h_{ii}}}$
>
> $\hat{Y_{i(i)}}$: i번째 관측치를 제외하고 적합시킨 회귀모델의 적합값(fitted value)
>
> $MSE_{(i)}$: i번째 관측치를 제외하고 회귀모델을 적합시켰을 때의 MSE

**DFBETAS(difference in beta(s))**: i번째 관측치를 제거했을 때, 회귀계수의 변동량

> $DFBETAS_{ij} = \frac{\hat{\beta_j} - \hat{\beta_{(i))j}}}{\sqrt{MSE_{(i)}C_{jj}}}$
>
> $C_{jj}$: $(X^TX)^{-1}$의 대각 요소중 j번째 값

**[Ex.7]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px">

위 모델에서 영향점 분석을 해봅니다.
    
</div>

```{R}
# 선형 회귀 모델 적합
lm_model <- lm(yield ~ fruitset, data = df_berry2)

# 회귀선 데이터 생성
fruitset_range <- data.frame(fruitset = range(df_berry2$fruitset))
fruitset_range$yield <- predict(lm_model, newdata = fruitset_range)

# 산점도와 회귀선 그리기
ggplot(df_berry2, aes(x = fruitset, y = yield)) +
  geom_point(size = 3, alpha = 0.5) +
  geom_line(data = fruitset_range, aes(y = yield), color = 'orange') +
  labs(title = "Fruitset vs Yield with Regression Line", x = "Fruitset", y = "Yield") +
  theme_minimal()
```

**Leverage**

$h_{ii} = x_i^T(X^TX)^{-1}x_i$, $X$는 디자인 행렬, $x_i$는 i번째 관측치에 해당하는 벡터

$h$: i번째 요소가 $h_{ii}$ 인 벡터, H를 구해 봅니다.


```{r}
# X 행렬 설정
X <- as.matrix(df_berry2 %>%
  select(fruitset) %>%
  mutate(const = 1))

# 레버리지 값 계산
H <- X %*% solve(t(X) %*% X) %*% t(X)
h <- diag(H)

# 결과 출력
h
```
**표준화 잔차를 구해봅니다.**

$t_i = \frac{\hat{e_i}}{\sqrt{MSE(1-h_{ii})}}$

$t_{std}$: i번째 요소가 $t_i$인 벡터를 만들어 봅니다.


```{r}
# 예측값 계산
s_prd <- predict(lm_model)

# 잔차 계산
residuals <- df_berry2$yield - s_prd

# SSE (Sum of Squared Residuals) 계산
sse <- sum(residuals^2)

# 회귀 모델의 자유도와 총 데이터 포인트 수
n <- nrow(df_berry2)

# MSE 계산
mse <- sse / (n - 2)

# X 행렬 설정
X <- as.matrix(df_berry2 %>%
  select(fruitset) %>%
  mutate(const = 1))

# 레버리지 값 계산
H <- X %*% solve(t(X) %*% X) %*% t(X)
h <- diag(H)

# 표준화된 잔차 계산
t_std <- residuals / sqrt(mse * (1 - h))

# 결과 출력
t_std
```

**DFFITs, DFBETAS를 구해봅니다.**

$DFFITs = \frac{\hat{Y_i} - \hat{Y_{i(i)}}}{\sqrt{MSE_{(i)}h_{ii}}}$

$DFBETAS_{ij} = \frac{\hat{\beta_j} - \hat{\beta_{(i))j}}}{\sqrt{MSE_{(i)}C_{jj}}}$

> $\hat{Y_{i(i)}}$: i번째 관측치를 제외하고 적합시킨 회귀모델의 적합값(fitted value)
>
> $MSE_{(i)}$: i번째 관측치를 제외하고 회귀모델을 적합시켰을 때의 MSE
>
$C_{jj}$: $(X^TX)^{-1}$의 대각 요소중 j번째 값


```{r}
# get_stat_without_i 함수 정의
get_stat_without_i <- function(df, x) {
  lm_model <- lm(yield ~ fruitset, data = df)
  MSE_i <- sum(lm_model$residuals^2) / (nrow(df) - 2)
  x_pred <- predict(lm_model, newdata = x)
  coefficients <- coef(lm_model)
  result <- c(MSE_i, x_pred, coefficients)
  names(result) <- c("MSE_i", "yii_hat", "Intercept", "fruitset")
  return(result)
}

# X 행렬 설정
X <- as.matrix(df_berry2 %>%
  select(fruitset) %>%
  mutate(const = 1))

# C 행렬 계산
C <- solve(t(X) %*% X)

# apply 함수 사용하여 각 데이터 포인트 제외한 통계 계산
df_fit_without_i <- df_berry2 %>%
  rowwise() %>%
  do(as.data.frame(t(get_stat_without_i(df_berry2[-which(df_berry2$yield == .$yield),], .))))

# 레버리지 값 계산
H <- X %*% solve(t(X) %*% X) %*% t(X)
h <- diag(H)
# dffits, dfbetas0, dfbetas1 계산
df_fit_without_i$dffits <- (s_prd - df_fit_without_i$yii_hat) / sqrt(df_fit_without_i$MSE_i * h)
df_fit_without_i$dfbetas0 <- (coef(lm_model)["(Intercept)"] - df_fit_without_i$Intercept) / sqrt(df_fit_without_i$MSE_i * C[2, 2])
df_fit_without_i$dfbetas1 <- (coef(lm_model)["fruitset"] - df_fit_without_i$fruitset) / sqrt(df_fit_without_i$MSE_i * C[1, 1])


# 결과 출력
print(head(df_fit_without_i))
```
```{r}
# 이상점 판단 지표 계산
influence_measures <- influence.measures(lm_model)

# studentized residuals, dffits, dfbetas 추출
df_influence <- as.data.frame(influence_measures$infmat) %>%
  select(dffit, starts_with("dfb.")) %>%
  rename(
    `dffits` = dffit,
    `dfbetas0` = dfb.1_,
    `dfbetas1` = dfb.frts
  )

# 결과 출력
print(df_influence)
```
```{r}
df_berry3 <- df_berry2 %>%
  mutate(
    dffits = df_influence$dffits,
    dfbetas1 = df_influence$dfbetas1
  )

# 회귀선 데이터 생성
fruitset_range <- data.frame(fruitset = range(df_berry3$fruitset))
fruitset_range$yield <- predict(lm_model, newdata = fruitset_range)

# 플롯 생성
plots <- list()

for (i in c("dffits", "dfbetas1")) {
  p <- ggplot(df_berry3, aes_string(x = "fruitset", y = "yield", color = i)) +
    geom_point(size = 3, alpha = 0.5) +
    geom_line(data = fruitset_range, aes(y = yield), color = 'orange') +
    labs(title = i, x = "fruitset", y = "yield") +
    theme_minimal()
  plots[[i]] <- p
}

# 플롯 배열
library(gridExtra)
grid.arrange(grobs = plots, ncol = 2)
```

**Cook's Distance**

$D_i = \frac{\sum_{j=1}^{n}\left(\hat{y_j} - \hat{y_{j(i)}}\right)^2}{pMSE}$

> $\hat{y_{j(i)}}$: i를 제외한 회귀 모델의 예측값
>
> $p$: 회귀 모델의 모수의 수

```{r}
# 전체 모델 적합
lm_model_full <- lm(yield ~ fruitset, data = df_berry2)
s_prd <- predict(lm_model_full)

# get_cooks_num 함수 정의
get_cooks_num <- function(df) {
  lm_model <- lm(yield ~ fruitset, data = df)
  sum((s_prd - predict(lm_model, newdata = df_berry2))^2)
}

# Cook's distance의 분자 항 계산
cooks_num <- df_berry2 %>%
  rowwise() %>%
  mutate(num = get_cooks_num(df_berry2[-which(df_berry2$yield == yield), ])) %>%
  ungroup()

# Cook's distance 계산
cooks_distance <- cooks_num$num / (summary(lm_model_full)$sigma^2 * 2)

# 결과 출력
print(cooks_distance)
```

### 다중 공선성(multicolinearity)

- 다중공선성은 다중 선형 회귀 모델의 독립 변수간에 발생하는 문제입니다.

- 다중 선형 회귀에서 독립변수들 간에 강한 선형관계가 존재하는 경우를 말합니다.

#### 다중 선형 회귀(Multiple Linear Regression)

- 독립 변수의 수가 여러  개인 선형 회귀 모델

※ 다중 선형 회귀 분석의 표준화 또는 정규화

> 독립 변수의 변수폭(스케일, Scale)을 균일화를 시켜 계수의 크기로 종속변수에 주는 영향도의 비교를 용이하게 합니다.


#### 다중공선성의 영향

- 불안정한 모델: 회귀계수 추정량의 변동성이 커져, 데이터가 조금만 바뀌어도 변동성이 커집니다.


- 완전한 다중공선성: 독립변수들 간에 완벽한 선형 관계가 있는 경우

> OLS: $\hat{\beta}=(X^TX)^{-1}X^Ty$ 에서 $(X^TX)^{-1}$가 존재하지 않습니다. 
>
> 제대로된 회귀 계수를 얻을 수 없어, 반드시 해소해야 합니다.
>
> Ex) 범주형 변수를 가변수화 하면 이들 간에 완벽한 공선성을 지니게 됩니다. 가변수 중 하나를 제거합니다.


#### 다중공선성 진단

- 시각화: Ex) 독립 변수간에 산점도 출력, 


- 상관행렬 도출


- 독립 변수 마다 VIF(variance inflation factor) 측정

#### 해결 방안

- 변수 선택


- 차원 축소

**[Ex.8]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px">

df_berry2에서 yield 는 종속 변수 fruitset, fruitmass, seed은 독립 변수로 회귀 모델을 만들어 봅니다.

fruitset, fruitmass, seed를 표준화 합니다.
    
</div>

```{r}
# 열 선택 및 z-score 표준화
X_cols <- c('fruitmass', 'fruitset', 'seeds')
df_standardized <- df_berry2 %>%
  select(all_of(X_cols)) %>%
  mutate(across(everything(), scale))

# 상수항 추가
df_standardized <- df_standardized %>%
  mutate(const = 1) %>%
  select(const, everything())

# 결과 출력
print(head(df_standardized))
```


```{r}
# 종속 변수 설정
y <- df_berry2$yield

# 선형 회귀 모델 적합
lm_model2 <- lm(y ~ ., data = df_standardized)

# 요약 정보 출력
summary(lm_model2)
```
#### VIF(Variance Inflation Factor)의 계산

1. 각 독립 변수 $X_i$에 에 대해, $X_i$를 종속 변수로 하고 나머지 독립 변수들을 설명 변수로 하는 회귀 분석을 수행합니다.


2. 이 회귀 분석의 결정 계수 $R_i^2$ 를 계산합니다.


3. $VIF_i$는 $R_i^2$를 사용하여 다음과 같이 계산됩니다


> $VIF_i = \frac{1}{1 - R_i^2}$

**[Ex.9]**

<div style="border: 1px solid #ddd; padding: 12px; margin-top: 10px">

df_berry2에서 yield 는 종속 변수 fruitset, fruitmass, seed은 독립 변수로 회귀 모델에서의 
    
다중 공선성 현상을 조사합니다.
    
그리고 VIF를 기준으로 변수를 선택하여 해소하는 방법과,
    
PCA를 사용하여 해소하는 방법을 보입니다.
    
</div>

```{r}
# 열 선택
X_cols <- c('fruitmass', 'fruitset', 'seeds')

# 산점도 생성
plots <- list()
combinations <- combn(X_cols, 2, simplify = FALSE)

for (comb in combinations) {
  p <- ggplot(df_berry2, aes_string(x = comb[1], y = comb[2])) +
    geom_point() +
    labs(title = paste(comb[1], "vs", comb[2]), x = comb[1], y = comb[2]) +
    theme_minimal()
  plots[[paste(comb[1], comb[2], sep = "_")]] <- p
}

# 플롯 배열
grid.arrange(grobs = plots, ncol = 3)
```

```{r}
library(car)

# VIF 계산을 위한 선형 회귀 모델 적합
lm_model <- lm(as.formula(paste("const ~", paste(X_cols, collapse = " + "))), data = df_standardized)

# VIF 계산
vifs <- vif(lm_model)

# VIF 값과 가장 큰 VIF 값을 가진 변수 출력
print(vifs)
print(names(vifs)[which.max(vifs)])
```
```{r}
# 열 선택 및 z-score 표준화
X_cols <- c('fruitset', 'seeds')
df_standardized <- df_berry2 %>%
  select(all_of(X_cols)) %>%
  mutate(across(everything(), scale))

# 상수항 추가
df_standardized <- df_standardized %>%
  mutate(const = 1) %>%
  select(const, everything())

# 종속 변수 설정
y <- df_berry2$yield

# 선형 회귀 모델 적합
lm_model3 <- lm(y ~ ., data = df_standardized)

# 요약 정보 출력
summary(lm_model3)
```
```{r}
# 열 선택 및 z-score 표준화
X_cols <- c('fruitset', 'seeds')
df_standardized <- df_berry2 %>%
  select(all_of(X_cols)) %>%
  mutate(across(everything(), scale))

# 상수항 추가
df_standardized <- df_standardized %>%
  mutate(const = 1) %>%
  select(const, everything())

# VIF 계산을 위한 선형 회귀 모델 적합
lm_model <- lm(as.formula(paste("const ~", paste(X_cols, collapse = " + "))), data = df_standardized)

# VIF 계산
vifs <- vif(lm_model)

# VIF 값 출력
vifs
```

```{r}
# 선형 회귀 모델 적합
lm_model3 <- lm(yield ~ fruitset, data = df_berry2)

# 요약 정보 추출
summary_lm3 <- summary(lm_model3)

# 조정된 결정 계수(R-squared adjusted) 출력
summary_lm3$adj.r.squared
```

```{r}
# 선형 회귀 모델 적합
lm_model3 <- lm(yield ~ seeds, data = df_berry2)

# 요약 정보 추출
summary_lm3 <- summary(lm_model3)

# 조정된 결정 계수(R-squared adjusted) 출력
summary_lm3$adj.r.squared
```
```{r}
# 선형 회귀 모델 적합
lm_model3 <- lm(yield ~ seeds, data = df_berry2)

# 요약 정보 출력
summary(lm_model3)
```
```{r}
# 열 선택 및 z-score 표준화
X_cols <- c('fruitmass', 'fruitset', 'seeds')
df_standardized <- df_berry2 %>%
  select(all_of(X_cols)) %>%
  mutate(across(everything(), scale))

# PCA 수행
pca_result <- prcomp(df_standardized, scale = TRUE)

# 설명된 분산 비율 출력
explained_variance_ratio <- summary(pca_result)$importance[2, ]
explained_variance_ratio
```

```{r}
# PCA 수행
pca_result <- prcomp(df_standardized, scale. = TRUE)
X_pca <- pca_result$x[, 1]

# 종속 변수 설정
y <- df_berry2$yield

# 주성분을 사용한 데이터프레임 생성
df_pca <- data.frame(X_pca, y)

# 선형 회귀 모델 적합
lm_model3 <- lm(y ~ X_pca, data = df_pca)

# 요약 정보 출력
summary(lm_model3)
```

